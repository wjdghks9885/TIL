# Deep Learning의 역사

> 간략하게 적겠다.

- 뉴런의 동작 방식을 이용하여 Neural Network(nn)을 개발하는게 시초가 된다.
- OR문제와 AND문제는 linear하게 해결이 됐었다. (다른 머신러닝 모델로도 가능)
- 하지만 XOR문제는 해결이 불가능했다. (nn으로도 불가능)
- 그래서 nn은 침체기를 맞았다.
- 그러던 중, 1986년 힌튼 교수가 Backpropagation(역전파)을 개발해낸다.
- 이 방법으로 nn이 다시 빛을 발하나 했지만...
- Backpropagation은 결과로부터 나온 에러를 이용해 다시 뒤로 가면서 가중치를 조정해가는 방법인데 layer가 많아지면 뒤로 갈수록 가중치를 조정하는 숫자가 매우 작아져서 사용할 수 없게 되었다. (Gradient vanishing)
- 그렇게 nn은 다시 침체기를 맞았다..

---

- 이를 해결할 방법을 찾기위해 여러 사람들이 갖은 노력을 했지만 결국 찾아내지 못하고 많은 시간이 흐른다. (약 30년 정도)

- 그러던 중, 2006~2007년 경 힌튼 교수와 벤지오 교수가 nn을 Deep Nets 혹은 Deep Learning이라고 부르자고 한다.
- 그렇게 사람들은 다시 관심을 갖게 되고, 다시 연구를 시작하게 된다.
  - 정말 주목을 받게 된 계기는 IMAGENET이라는 챌린지로 시작 된다.
  - (이미지를 주고 컴퓨터로 무슨 이미지인지 맞추는 챌린지)
  - 2015년에 사람보다 더 잘 맞추게 되는 경지에 이르른다.

---

- 2010년 경 사람들은 위에서 언급한 Gradient vanishing현상을 해결할 수 있는 방법을 찾아낸다!
- 지금까지는 활성화함수를 sigmoid를 거의 썼었는데 이것이 문제였던것이다.
  - 어떤 값이든 sigmoid를 거치면 0 ~ 1까지의 값만 갖게되었다.
  - 그래서 계속 곱해나가면 점점 작은 값이 되기 때문에 vanishing문제가 생겼던 것이다.
- 이를 해결할 수 있는 ReLU라는 활성화함수를 발명해낸다.
  - ReLU는 0이하의 값은 0으로, 0보다 큰 값은 그대로 가져가는 함수이다.
  - 그래서 큰 값이 들어오면 그대로 큰 값을 가지고 가서 곱했을 때 엄청 작아지는 현상을 막게 되었다.

<br>

- 이 이후로 딥러닝이 점점 발달하게 되었고, 현재까지 발전 중이다.